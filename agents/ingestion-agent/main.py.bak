import os
import json
import base64
import asyncio
import logging
import dataclasses
import re
from typing import Any, List, Optional, Dict
from google.cloud import pubsub_v1
import functions_framework
from google.cloud import firestore

# ============================================================================
# BEGINN: INLINED CODE aus shared/simplified_ingestion
# ============================================================================

# ----------------------------------------------------------------------------
# INLINED: models.py
# ----------------------------------------------------------------------------
from pydantic import BaseModel, Field, field_validator, model_validator
from typing import List, Optional, Dict, Any, Union
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class BookIngestionRequest(BaseModel):
    book_id: str = Field(..., description="Firestore Document ID")
    user_id: str = Field(..., description="User ID f√ºr Multi-Tenancy")
    image_urls: List[str] = Field(
        ...,
        min_length=1,
        max_length=10,
        description="Liste von Bild-URLs (max 10)"
    )
    session_id: Optional[str] = Field(None, description="Optional Session ID f√ºr Tracking")

    @field_validator('image_urls')
    @classmethod
    def validate_image_urls(cls, v: List[str]) -> List[str]:
        if not v:
            raise ValueError("image_urls darf nicht leer sein")
        for url in v:
            if not url.startswith(('http://', 'https://', 'gs://', 'file://')):
                raise ValueError(f"Ung√ºltige URL: {url}")
        return v

class BookData(BaseModel):
    title: Optional[str] = Field(None, description="Buchtitel")
    authors: List[str] = Field(default_factory=list, description="Liste der Autoren")
    isbn_13: Optional[str] = Field(None, description="ISBN-13")
    isbn_10: Optional[str] = Field(None, description="ISBN-10")
    publisher: Optional[str] = Field(None, description="Verlag")
    publication_year: Optional[int] = Field(None, ge=1000, le=2100, description="Erscheinungsjahr")
    edition: Optional[str] = Field(None, description="Edition/Auflage")
    language: str = Field("de", description="Sprache (ISO 639-1 Code)")
    page_count: Optional[int] = Field(None, ge=1, description="Seitenzahl")
    genre: List[str] = Field(default_factory=list, description="Genre/Gattung")
    categories: List[str] = Field(default_factory=list, description="Kategorien")
    description: Optional[str] = Field(None, description="Buchbeschreibung")
    cover_url: Optional[str] = Field(None, description="URL zum Cover-Bild")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Zus√§tzliche Metadaten und Debug-Infos")

    @field_validator('isbn_13')
    @classmethod
    def validate_isbn_13(cls, v: Optional[str]) -> Optional[str]:
        if v is None:
            return v
        cleaned = v.replace('-', '').replace(' ', '')
        if len(cleaned) != 13 or not cleaned.isdigit():
            logger.warning(f"Invalid ISBN-13 format: {v}")
            return None
        return cleaned

    @field_validator('isbn_10')
    @classmethod
    def validate_isbn_10(cls, v: Optional[str]) -> Optional[str]:
        if v is None:
            return v
        cleaned = v.replace('-', '').replace(' ', '')
        if len(cleaned) != 10:
            logger.warning(f"Invalid ISBN-10 format: {v}")
            return None
        return cleaned

class GroundingMetadata(BaseModel):
    search_active: bool = Field(False, description="Ob Google Search aktiv war")
    queries_used: List[str] = Field(default_factory=list, description="Verwendete Suchanfragen")
    source_urls: List[str] = Field(default_factory=list, description="Verwendete Quellen-URLs")

def find_and_extract_book_data(data: Union[Dict, Any]) -> Optional[Dict]:
    if not isinstance(data, dict):
        return None
    for key, value in data.items():
        if key in ("book_identification", "book_data"):
            if isinstance(value, dict):
                return value
        if isinstance(value, dict):
            result = find_and_extract_book_data(value)
            if result:
                return result
    return None

class BookIngestionResult(BaseModel):
    success: bool = Field(..., description="Ob die Identifikation erfolgreich war")
    book_data: Optional[BookData] = Field(None, description="Extrahierte Buchdaten")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence Score")
    sources_used: List[str] = Field(default_factory=list, description="Verwendete Quellen")
    processing_time_ms: float = Field(..., ge=0, description="Verarbeitungszeit in ms")
    grounding_metadata: GroundingMetadata = Field(default_factory=GroundingMetadata, description="Google Search Grounding Metadata")
    timestamp: datetime = Field(default_factory=datetime.utcnow, description="Zeitstempel")

    @model_validator(mode='before')
    @classmethod
    def flatten_gemini_response(cls, data: Any) -> Any:
        if not isinstance(data, dict):
            return data
        
        # WICHTIG: Wenn book_data bereits ein BookData Objekt ist (von model_construct),
        # dann NICHT √ºberschreiben oder neu parsen!
        if 'book_data' in data and isinstance(data['book_data'], BookData):
            logger.info("‚úÖ book_data ist bereits ein BookData Objekt - √ºberspringe Validierung")
            return data
            
        logger.debug(f"BookIngestionResult Rohdaten: {data}")
        
        # Nur bei dict oder None weiter verarbeiten
        if 'book_data' in data and not isinstance(data['book_data'], dict):
            if data['book_data'] is not None:
                logger.warning(f"‚ö†Ô∏è book_data hat unerwarteten Typ: {type(data['book_data'])}")
            data['book_data'] = None
            
        if not data.get('book_data'):
            extracted_data = find_and_extract_book_data(data)
            if extracted_data:
                logger.info(f"Buchdaten extrahiert aus verschachtelter Antwort: {extracted_data}")
                data['book_data'] = extracted_data
            else:
                logger.warning("Konnte 'book_data' oder 'book_identification' nicht in der Antwort finden.")
        return data

    def get_firestore_status(self, threshold: float = 0.7) -> str:
        return "ingested" if self.confidence >= threshold else "needs_review"

class IngestionError(BaseModel):
    error_type: str = Field(..., description="Fehler-Typ (z.B. API_ERROR, VALIDATION_ERROR)")
    error_message: str = Field(..., description="Detaillierte Fehlermeldung")
    book_id: str = Field(..., description="Betroffene Buch-ID")
    user_id: str = Field(..., description="Betroffene User-ID")
    timestamp: datetime = Field(default_factory=datetime.utcnow, description="Zeitstempel")
    retry_possible: bool = Field(True, description="Ob ein Retry sinnvoll ist")
    gemini_error_code: Optional[str] = Field(None, description="Gemini API Error Code")
    grounding_failed: bool = Field(False, description="Ob Grounding fehlgeschlagen ist")
    image_count: int = Field(0, ge=0, description="Anzahl der verarbeiteten Bilder")

# ----------------------------------------------------------------------------
# INLINED: config.py
# ----------------------------------------------------------------------------
SYSTEM_INSTRUCTIONS = """
Du bist ein Experte f√ºr Bucherkennung und den deutschen Buchmarkt.
Deine Aufgabe:
Analysiere die bereitgestellten Buchbilder und nutze Google Search, um das Buch exakt zu identifizieren
und ALLE bibliografischen Metadaten zu extrahieren.
WICHTIG:
- KEINE Analyse von Zustand, Preisen oder Verf√ºgbarkeit durchf√ºhren.
- Fokus liegt rein auf der korrekten Identifikation der Ausgabe (Metadaten).
Qualit√§tsstandards:
- Nutze Google Search zur Verifizierung der bibliografischen Daten (Existenz, Verlag, Jahr)
- Verifiziere Informationen mit mehreren Quellen
- Gib realistische Confidence-Scores (0.7-0.95 ist normal)
- Bei Unsicherheit: lieber "needs_review" als falsche Daten
Fokus Deutsche M√§rkte:
- Nutze Quellen wie DNB, eurobuch.de, Amazon.de, Thalia, ZVAB zur Verifizierung der Metadaten
- Deutsche Verlage und Ausgaben priorisieren
"""

TASK_PROMPT_TEMPLATE = """
Analysiere diese Buchbilder und extrahiere ALLE Metadaten.
Nutze Google Search um:
1. **Basis-Identifikation**
   - ISBN (wenn sichtbar)
   - Titel (exakt)
   - Autor(en)
2. **Editions-Details**
   - Welche Edition/Ausgabe? (Taschenbuch, Gebunden, Sonderausgabe)
   - Erscheinungsjahr dieser Edition
   - Verlag und Auflage
   - Besondere Merkmale (Cover-Variante, Extras)
3. **Zus√§tzliche Metadaten**
   - Genre/Kategorien
   - Seitenzahl
   - Sprache
   - Beschreibung (kurz)
   - Cover-URL (falls verf√ºgbar)
Analysiere die Bilder gr√ºndlich:
- Cover (Vorder- und R√ºckseite)
- Impressum (Copyright-Seite)
- Buchr√ºcken
BITTE BEACHTEN:
- Ignoriere den Zustand des Buches.
- Suche NICHT nach Preisen oder Verf√ºgbarkeit.
- Konzentriere dich ausschlie√ülich auf die statischen Buch-Metadaten.
Gib das Ergebnis als JSON zur√ºck (siehe Schema).
"""

JSON_RESPONSE_SCHEMA = {
    "type": "object",
    "properties": {
        "success": {"type": "boolean", "description": "Ob die Identifikation erfolgreich war"},
        "book_data": {
            "type": "object", "nullable": True,
            "properties": {
                "title": {"type": "string", "description": "Buchtitel"},
                "authors": {"type": "array", "items": {"type": "string"}, "description": "Liste der Autoren"},
                "isbn_13": {"type": "string", "nullable": True, "description": "ISBN-13"},
                "isbn_10": {"type": "string", "nullable": True, "description": "ISBN-10"},
                "publisher": {"type": "string", "nullable": True, "description": "Verlag"},
                "publication_year": {"type": "integer", "nullable": True, "description": "Erscheinungsjahr"},
                "edition": {"type": "string", "nullable": True, "description": "Edition/Auflage"},
                "language": {"type": "string", "description": "Sprache (ISO 639-1)"},
                "page_count": {"type": "integer", "nullable": True, "description": "Seitenzahl"},
                "genre": {"type": "array", "items": {"type": "string"}, "description": "Genre/Gattung"},
                "categories": {"type": "array", "items": {"type": "string"}, "description": "Kategorien"},
                "description": {"type": "string", "nullable": True, "description": "Buchbeschreibung"},
                "cover_url": {"type": "string", "nullable": True, "description": "URL zum Cover-Bild"},
            },
            "required": ["title"]
        },
        "confidence": {"type": "number", "description": "Gesamte Confidence der Identifikation"},
        "sources_used": {"type": "array", "items": {"type": "string"}, "description": "Verwendete Quellen (z.B. Google Books, Amazon.de)"},
    },
    "required": ["success", "confidence"]
}

class GeminiResponse(BaseModel):
    success: bool
    book_data: Optional[BookData]
    confidence: float
    sources_used: List[str]

@dataclasses.dataclass
class IngestionConfig:
    model: str = "gemini-2.5-pro"
    temperature: float = 0.1
    max_output_tokens: int = 4096
    retry_attempts: int = 3  # Erh√∂ht auf 3 um 429 Fehler abzufedern
    retry_delay_seconds: float = 4.0  # Startverz√∂gerung erh√∂ht
    retry_exponential_base: float = 2.0

DEFAULT_CONFIG = IngestionConfig()


# ----------------------------------------------------------------------------
# INLINED: core.py (Vollst√§ndige Logik aus core.py zur Fehlerbehebung)
# ----------------------------------------------------------------------------
import time
from pathlib import Path
from urllib.parse import urlparse, unquote
import requests
from io import BytesIO

try:
    from google import genai
    from google.genai import types
except ImportError:
    raise ImportError("google-genai is required. Install with: pip install 'google-genai>=0.3.0'")

class IngestionException(Exception):
    def __init__(self, error: IngestionError):
        self.error = error
        super().__init__(f"{error.error_type}: {error.error_message}")

def prepare_images(image_urls: List[str]) -> List[types.Part]:
    """Bereitet Bilder f√ºr Gemini vor."""
    # print(f"!!! prepare_images: Verarbeite {len(image_urls)} URLs")
    parts = []
    for url in image_urls:
        try:
            if url.startswith('gs://'):
                # print(f"!!! prepare_images: Lade GCS Bild: {url}")
                parts.append(types.Part.from_uri(file_uri=url, mime_type="image/jpeg"))
            elif url.startswith(('http://', 'https://')):
                # print(f"!!! prepare_images: Lade HTTP Bild: {url}")
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                parts.append(types.Part.from_bytes(data=response.content, mime_type="image/jpeg"))
            elif url.startswith('file://'):
                # print(f"!!! prepare_images: Lade lokales Bild: {url}")
                parsed = urlparse(url)
                path_str = unquote(parsed.path)
                if os.name == 'nt' and path_str.startswith('/'):
                    path = Path(path_str[1:])
                else:
                    path = Path(path_str)
                with open(path, 'rb') as f:
                    image_bytes = f.read()
                parts.append(types.Part.from_bytes(data=image_bytes, mime_type="image/jpeg"))
        except Exception as e:
            # print(f"!!! prepare_images: FEHLER bei {url}: {e}")
            logging.error(f"Fehler beim Laden von {url}: {e}")
    if not parts:
        raise ValueError("Keine g√ºltigen Bilder gefunden")
    # print(f"!!! prepare_images: {len(parts)} Bilder erfolgreich vorbereitet")
    return parts

def extract_grounding_metadata(response: Any) -> GroundingMetadata:
    """Extrahiert Grounding Metadata aus Gemini Response."""
    # print("!!! extract_grounding_metadata: Extrahiere Metadaten...")
    metadata = GroundingMetadata()
    try:
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            # Das Attribut hei√üt jetzt 'grounding_attributions' und nicht 'citation_metadata'
            if hasattr(candidate, 'grounding_attributions') and candidate.grounding_attributions:
                attributions = candidate.grounding_attributions
                if attributions:
                    metadata.search_active = True
                    for attr in attributions:
                        if hasattr(attr, 'web') and hasattr(attr.web, 'uri'):
                            metadata.source_urls.append(attr.web.uri)
                    if metadata.source_urls:
                        metadata.queries_used = ["Google Search was used (query not available)"]
        # print(f"!!! extract_grounding_metadata: search_active={metadata.search_active}, sources={len(metadata.source_urls)}")
    except Exception as e:
        # print(f"!!! extract_grounding_metadata: FEHLER: {e}")
        logging.error(f"Error extracting grounding metadata: {e}", exc_info=True)
    return metadata

async def ingest_book_with_gemini(
    request: BookIngestionRequest,
    config: IngestionConfig = DEFAULT_CONFIG,
    system_instructions: Optional[str] = None,
    task_prompt: Optional[str] = None
) -> BookIngestionResult:
    """Inlined Ingestion Logik mit verst√§rktem Logging und deaktiviertem Search Grounding."""
    # print(f"!!! ingest_book_with_gemini: START f√ºr Book ID {request.book_id}")
    start_time = time.time()
    
    if system_instructions is None:
        system_instructions = SYSTEM_INSTRUCTIONS
    if task_prompt is None:
        task_prompt = TASK_PROMPT_TEMPLATE

    try:
        project_id = os.environ.get("GCP_PROJECT") or os.environ.get("GOOGLE_CLOUD_PROJECT")
        location = os.environ.get("GCP_REGION", "us-central1")
        api_key = os.environ.get("GEMINI_API_KEY")
        
        if project_id:
            # print(f"!!! Gemini Client: Initialisiere mit Vertex AI (project={project_id}, location={location})")
            client = genai.Client(
                vertexai=True,
                project=project_id,
                location=location
            )
        elif api_key:
            # print(f"!!! Gemini Client: Initialisiere mit API Key")
            client = genai.Client(api_key=api_key)
        else:
            raise ValueError("Weder GCP_PROJECT noch GEMINI_API_KEY gefunden.")

        image_parts = prepare_images(request.image_urls)
        
        model_name = config.model
        if not api_key and not model_name.startswith("publishers/"):
            model_name = config.model
        
        # print(f"!!! Generation Config: Model={model_name}, SearchGrounding=DISABLED, Safety=BLOCK_NONE, JSON Schema=ENABLED")
        generate_content_config = types.GenerateContentConfig(
            temperature=config.temperature,
            max_output_tokens=config.max_output_tokens,
            system_instruction=system_instructions,
            response_mime_type="application/json",
            response_schema=JSON_RESPONSE_SCHEMA,
            safety_settings=[
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                    threshold=types.HarmBlockThreshold.BLOCK_NONE,
                ),
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,
                    threshold=types.HarmBlockThreshold.BLOCK_NONE,
                ),
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                    threshold=types.HarmBlockThreshold.BLOCK_NONE,
                ),
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                    threshold=types.HarmBlockThreshold.BLOCK_NONE,
                ),
                types.SafetySetting(
                    category=types.HarmCategory.HARM_CATEGORY_CIVIC_INTEGRITY,
                    threshold=types.HarmBlockThreshold.BLOCK_NONE,
                ),
            ]
        )
        
        contents = image_parts + [task_prompt]

        # print("!!! API CALL: Sende Anfrage an client.models.generate_content...")
        try:
            response = client.models.generate_content(
                model=config.model,
                contents=contents,
                config=generate_content_config
            )
            # print(f"!!! API CALL: Antwort empfangen. Typ: {type(response)}")
            logger.info(f"üì• FULL GEMINI RESPONSE TYPE: {type(response)}")
            # logger.info(f"üì• RESPONSE DIR: {dir(response)}")
            
            # Log Response-Struktur detailliert
            # if hasattr(response, '__dict__'):
            #    logger.info(f"üì• RESPONSE __dict__: {response.__dict__}")
            
        except Exception as e:
            # print(f"!!! API CALL FEHLER: {e}")
            logger.error(f"‚ùå API CALL FEHLER: {e}", exc_info=True)
            raise e
        
        # print("!!! Parsing: Extrahiere Text aus Response...")
        logger.info("üîç Parsing: Extrahiere Text aus Response...")
        
        if hasattr(response, 'candidates') and response.candidates:
            cand = response.candidates[0]
            # print(f"!!! Candidate 0 Finish Reason: {getattr(cand, 'finish_reason', 'N/A')}")
            logger.info(f"üìä Candidate 0 Finish Reason: {getattr(cand, 'finish_reason', 'N/A')}")
            
            # Log Candidate-Struktur
            if hasattr(cand, 'content'):
                logger.info(f"üìä Candidate has content: {hasattr(cand.content, 'parts')}")
                if hasattr(cand.content, 'parts') and cand.content.parts:
                    logger.info(f"üìä Number of parts: {len(cand.content.parts)}")
                    # for i, part in enumerate(cand.content.parts):
                    #    logger.info(f"üìä Part {i} attributes: {dir(part)}")

        result_text = ""
        try:
            result_text = response.text
        except Exception as e:
            # print(f"!!! Fehler beim Zugriff auf response.text: {e}")
            if hasattr(response, 'candidates') and response.candidates and \
               response.candidates[0].content and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'text') and part.text:
                        result_text = part.text
                        break
        
        if not result_text.strip():
            # print("!!! Parsing: KEIN TEXT GEFUNDEN! Gemini Antwort war leer.")
            raise json.JSONDecodeError("Leere Antwort von Gemini", "", 0)

        # print(f"!!! Parsing: Result Text L√§nge = {len(result_text)}")
        logger.info(f"üìù Result Text (first 500 chars): {result_text[:500]}")
        
        try:
            match = re.search(r"(\{.*\})", result_text, re.DOTALL)
            if match:
                json_str = match.group(1)
                result_json = json.loads(json_str)
                # print("!!! Parsing: JSON erfolgreich via Regex extrahiert und geladen")
                logger.info("‚úÖ JSON erfolgreich via Regex extrahiert")
            else:
                clean_text = re.sub(r"^```json\s*", "", result_text, flags=re.MULTILINE)
                clean_text = re.sub(r"^```\s*", "", clean_text, flags=re.MULTILINE)
                clean_text = re.sub(r"```$", "", clean_text, flags=re.MULTILINE).strip()
                result_json = json.loads(clean_text)
                # print("!!! Parsing: Markdown-bereinigter Text als JSON geladen")
                logger.info("‚úÖ Markdown-bereinigter Text als JSON geladen")
                
            # Log vollst√§ndigen JSON
            # logger.info(f"üìã FULL PARSED JSON: {json.dumps(result_json, indent=2, ensure_ascii=False)}")
            logger.info(f"üìã JSON Top-level keys: {list(result_json.keys())}")

        except json.JSONDecodeError as e:
            # print(f"!!! Parsing: JSON DECODE ERROR: {e}")
            # print(f"!!! Parsing: Problematischer Text (komplett): {result_text}")
            logger.error(f"‚ùå JSON DECODE ERROR: {e}")
            logger.error(f"‚ùå Problematischer Text: {result_text}")
            raise e

        grounding_metadata = extract_grounding_metadata(response)
        
        # Gemini liefert bereits das korrekte Format direkt, nutze es
        book_data = None
        book_data_dict = result_json.get("book_data") if isinstance(result_json, dict) else None
        
        logger.info(f"üîç Extracted book_data from result_json: {book_data_dict is not None}")
        if book_data_dict and isinstance(book_data_dict, dict):
            logger.info(f"üìö book_data keys: {list(book_data_dict.keys())}")
            # print(f"!!! Buchdaten gefunden: {list(book_data_dict.keys())}")
            if "metadata" not in book_data_dict:
                book_data_dict["metadata"] = {}
            book_data_dict["metadata"]["raw_gemini_response"] = result_json
            try:
                book_data = BookData(**book_data_dict)
                logger.info(f"‚úÖ BookData successfully created with title: {book_data.title}")
            except Exception as ve:
                # print(f"!!! BookData Validation Error: {ve}")
                logger.error(f"‚ùå BookData Validation Error: {ve}", exc_info=True)
        else:
            logger.warning(f"‚ö†Ô∏è No valid book_data in result_json!")
            if isinstance(result_json, dict):
                logger.warning(f"‚ö†Ô∏è result_json keys: {list(result_json.keys())}")
        
        processing_time = (time.time() - start_time) * 1000
        
        # Der Erfolg h√§ngt jetzt prim√§r davon ab, ob wir Buchdaten extrahieren konnten.
        ingestion_success = book_data is not None
        
        # print(f"!!! ingest_book_with_gemini: ERFOLG (technisch) f√ºr {request.book_id} in {processing_time:.0f}ms. Ingestion Success: {ingestion_success}")
        
        # WICHTIG: Direkt das BookData Objekt √ºbergeben, nicht √ºber den model_validator laufen lassen
        # Der model_validator w√ºrde die Daten nochmal parsen und dabei √ºberschreiben
        result = BookIngestionResult.model_construct(
            success=ingestion_success,
            book_data=book_data,
            confidence=result_json.get("confidence", 0.0) if isinstance(result_json, dict) and book_data else 0.0,
            sources_used=result_json.get("sources_used", []) if isinstance(result_json, dict) else [],
            processing_time_ms=processing_time,
            grounding_metadata=grounding_metadata,
            timestamp=datetime.utcnow()
        )
        
        logger.info(f"üéØ BookIngestionResult created - success: {result.success}, has_book_data: {result.book_data is not None}")
        if result.book_data:
            logger.info(f"üéØ Final book_data title: {result.book_data.title}")
        
        return result
    except Exception as e:
        # print(f"!!! ingest_book_with_gemini: KRITISCHER FEHLER: {e}")
        raise IngestionException(IngestionError(
            error_type=type(e).__name__,
            error_message=str(e),
            book_id=request.book_id,
            user_id=request.user_id,
            retry_possible='rate' in str(e).lower() or 'quota' in str(e).lower() or '429' in str(e) or 'exhausted' in str(e).lower()
        ))

async def ingest_book_with_retry(
    request: BookIngestionRequest,
    config: IngestionConfig = DEFAULT_CONFIG,
) -> BookIngestionResult:
    """Wrapper mit Retry Logik."""
    # print(f"!!! ingest_book_with_retry: Book ID {request.book_id}, Max Retries {config.retry_attempts}")
    for attempt in range(config.retry_attempts + 1):
        try:
            if attempt > 0:
                # print(f"!!! ingest_book_with_retry: RETRY Versuch {attempt}")
                pass
            return await ingest_book_with_gemini(request, config)
        except IngestionException as e:
            if not e.error.retry_possible or attempt >= config.retry_attempts:
                # print(f"!!! ingest_book_with_retry: Kein weiterer Retry f√ºr {request.book_id}")
                raise
            delay = config.retry_delay_seconds * (config.retry_exponential_base ** attempt)
            # print(f"!!! ingest_book_with_retry: Warte {delay:.1f}s vor n√§chstem Versuch...")
            await asyncio.sleep(delay)
    raise IngestionException(IngestionError(error_type="UNKNOWN_ERROR", error_message="Max retries reached", book_id=request.book_id, user_id=request.user_id))


# ============================================================================
# ENDE: INLINED CODE
# ============================================================================


# Konfiguriere Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# VERSION MARKER FOR DEPLOYMENT DEBUGGING
logger.info("**************************************************")
logger.info("VERSION MARKER: v2.24-DIRECT-BOOK-DATA")
# print("!!! VERSION MARKER: v2.24-DIRECT-BOOK-DATA")
logger.info("**************************************************")

# Firestore Client Initialisierung (dynamisch)
project_id = os.environ.get("GCP_PROJECT")
db = firestore.Client(project=project_id)

def get_firestore_client():
    return db

def create_condition_assessment_request(uid, book_id, payload):
    db.collection('users').document(uid).collection('books').document(book_id).collection('conditionAssessments').add(payload)

# Function to get Project ID from environment or metadata server
def get_project_id():
    """Returns the Project ID from environment variables."""
    return os.environ.get("GCP_PROJECT", "project-52b2fab8-15a1-4b66-9f3")

# Initialize Pub/Sub client
try:
    project_id = get_project_id()
    if project_id:
        publisher = pubsub_v1.PublisherClient()
        topic_path = publisher.topic_path(project_id, "trigger-condition-assessment")
        logger.info(f"Pub/Sub publisher initialized for topic: {topic_path}")
    else:
        logger.critical("PROJECT_ID ist None. Cannot initialize Pub/Sub.")
        publisher = None
        topic_path = None
except Exception as e:
    logger.critical(f"Failed to initialize Pub/Sub publisher: {e}", exc_info=True)
    publisher = None
    topic_path = None


@functions_framework.cloud_event
def ingestion_analysis_agent(cloud_event: Any):
    """Wrapper f√ºr die Cloud Function."""
    try:
        asyncio.run(_async_ingestion_analysis_agent(cloud_event))
    except Exception as e:
        # Fange ALLE Fehler ab, um sicherzustellen, dass wir 200 OK zur√ºckgeben
        # und die Pub/Sub-Nachricht best√§tigen (ack), um Endlosschleifen zu vermeiden.
        logger.critical(f"Unhandled error in ingestion agent (caught in wrapper): {e}", exc_info=True)
        # Wir k√∂nnten hier auch versuchen, den Firestore-Status zu aktualisieren, 
        # aber ohne book_id/uid ist das schwer.
        
    # Immer OK zur√ºckgeben, damit Pub/Sub die Nachricht nicht erneut sendet.
    return "OK", 200


async def _async_ingestion_analysis_agent(cloud_event: Any) -> None:
    try:
        message_data = base64.b64decode(cloud_event.data["message"]["data"]).decode('utf-8')
        message_json = json.loads(message_data)
    except Exception as e:
        logger.error(f"Invalid message format: {e}")
        return

    book_id = message_json.get('bookId')
    image_urls = message_json.get('imageUrls')
    uid = message_json.get('uid')

    if not all([book_id, image_urls, uid]) or not isinstance(image_urls, list) or len(image_urls) == 0:
        logger.error(f"Missing required fields or empty imageUrls in message: {message_json}")
        return

    logger.info(f"üì® Received Pub/Sub message - bookId: {book_id}, uid: {uid}, images: {len(image_urls)}")
    logger.info(f"Processing book {book_id} for user {uid} with {len(image_urls)} images")
    book_ref = db.collection('users').document(uid).collection('books').document(book_id)

    @firestore.transactional
    def check_and_update_status(transaction):
        snapshot = book_ref.get(transaction=transaction)
        if snapshot.exists:
            status = snapshot.to_dict().get('status')
            logger.info(f"üìÑ Found existing document for {book_id} with status: {status}")
            # Nur bei finalen States √ºberspringen - erlaubt Retry bei pending_analysis
            if status in ['ingested', 'needs_review', 'analysis_failed', 'condition_assessed']:
                logger.warning(f"Book {book_id} already finished ({status}). Skipping.")
                return False
            # Erlaubt Retry bei 'pending_analysis' oder 'ingesting'
        else:
            logger.warning(f"‚ö†Ô∏è Document {book_id} does NOT exist in Firestore! This should not happen.")
        
        transaction.set(book_ref, {'status': 'ingesting'}, merge=True)
        logger.info(f"‚úÖ Updated status to 'ingesting' for {book_id}")
        return True

    transaction = db.transaction()
    should_process = check_and_update_status(transaction)

    if not should_process:
        return


    try:
        request = BookIngestionRequest(
            book_id=book_id,
            user_id=uid,
            image_urls=image_urls
        )
        
        result = await ingest_book_with_retry(request)
        
        if result.book_data:
            final_data = {
                "status": result.get_firestore_status(),
                "title": result.book_data.title,
                "authors": result.book_data.authors,
                "isbn": result.book_data.isbn_13 or result.book_data.isbn_10,
                "publisher": result.book_data.publisher,
                "publication_year": result.book_data.publication_year,
                "edition": result.book_data.edition,
                "language": result.book_data.language,
                "page_count": result.book_data.page_count,
                "genre": result.book_data.genre,
                "categories": result.book_data.categories,
                "cover_url": result.book_data.cover_url,
                "description": result.book_data.description,
                "confidence_score": result.confidence,
                "sources_used": result.sources_used,
                "_metadata": {
                    "processing_time_ms": result.processing_time_ms,
                    "simplified_ingestion": True,
                    "grounding_metadata": result.grounding_metadata.model_dump() if result.grounding_metadata else None,
                }
            }
            book_ref.update(final_data)
            logger.info(f"Simplified ingestion processed for book {book_id} with status {final_data['status']}")

            if publisher and topic_path:
                try:
                    payload = {"book_id": book_id, "user_id": uid, "image_urls": image_urls}
                    data = json.dumps(payload).encode("utf-8")
                    future = publisher.publish(topic_path, data)
                    future.result()
                    logger.info(f"‚úÖ Successfully published condition assessment job for book {book_id}")
                except Exception as e:
                    logger.error(f"‚ùå Failed to trigger condition assessment for book {book_id}: {e}", exc_info=True)
            else:
                logger.error("‚ùå Pub/Sub publisher not initialized. Cannot trigger condition assessment.")

        else:
            logger.warning(f"Ingestion for book {book_id} failed: Gemini returned no book data.")
            book_ref.update({
                'status': 'analysis_failed',
                'error_message': 'Gemini returned no book data.',
                'error_type': 'INGESTION_NO_DATA',
            })

    except IngestionException as e:
        logger.error(f"Simplified ingestion failed for book {book_id}: {e.error.error_message}")
        book_ref.update({
            'status': 'analysis_failed',
            'error_message': e.error.error_message,
            'error_type': e.error.error_type,
        })
    except Exception as e:
        logger.error(f"Unexpected error for book {book_id}: {e}", exc_info=True)
        book_ref.update({'status': 'analysis_failed', 'error_message': str(e)})
