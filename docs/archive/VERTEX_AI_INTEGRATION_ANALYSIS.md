# üöÄ Vertex AI Integration - Analyse & Implementierungsstrategie

> Erweiterte Datenanreicherung f√ºr die Intelligent Book Sales Pipeline mit Vertex AI Services

---

## üìã Executive Summary

**Ziel:** Erweitere die Data Fusion Engine mit intelligenten Vertex AI Services f√ºr pr√§zisere Buchdaten

**Empfohlene Services:**
1. ‚úÖ **Gemini-basierter Web Scraper** - Intelligentes Crawling von eurobuch.de, ZVAB, etc.
2. ‚ö†Ô∏è **Vertex AI Search** - M√∂glicherweise Overkill f√ºr unseren Use Case
3. ‚úÖ **Grounding with Google Search** - Bessere Alternative zu Vertex AI Search

**Empfehlung:** Start mit **Gemini Web Scraper + Google Search Grounding**

---

## üéØ Vertex AI Services im Detail

### 1. Gemini-basierter Web Scraper ‚≠ê EMPFOHLEN

**Was es ist:**
Intelligenter Crawler der mit Gemini 2.0 Flash Websites analysiert und strukturierte Daten extrahiert.

**Vorteile f√ºr unsere Pipeline:**
- ‚úÖ Kann komplexe Websites wie eurobuch.de verstehen
- ‚úÖ Extrahiert strukturierte Buchdaten aus HTML
- ‚úÖ Robust gegen Website-√Ñnderungen
- ‚úÖ Kann mehrere Quellen parallel crawlen
- ‚úÖ Kosten-effizient mit Gemini 2.0 Flash

**Anwendungsfall:**
```python
# Beispiel: Eurobuch.de nach Buch-ISBN suchen
isbn = "9783423282388"
url = f"https://www.eurobuch.de/buch/isbn/{isbn}.html"

# Gemini crawlt die Seite und extrahiert:
result = {
    "price_range": {"min": 4.99, "max": 12.50},
    "availability": "27 Angebote",
    "condition_stats": {"neu": 5, "gebraucht": 22},
    "average_price": 7.85,
    "publishers_found": ["dtv", "Goldmann"],
    "editions_found": ["Taschenbuch", "Gebunden"]
}
```

**M√∂gliche Quellen:**
- üìö eurobuch.de - Preis-Aggregator
- üìö zvab.com - Antiquarische B√ºcher
- üìö booklooker.de - Gebrauchtbuchmarkt
- üìö abebooks.de - Internationale Angebote
- üìö amazon.de - Marktpreise und Rezensionen

**Implementierungs-Komplexit√§t:** üü¢ Niedrig (2-3 Tage)

---

### 2. Grounding with Google Search ‚≠ê EMPFOHLEN

**Was es ist:**
Gemini-Feature das automatisch Google Search nutzt um aktuelle Informationen zu finden.

**Vorteile f√ºr unsere Pipeline:**
- ‚úÖ **Bereits in Gemini integriert** - keine separate API
- ‚úÖ Findet aktuelle Marktpreise
- ‚úÖ Entdeckt neue Editionen
- ‚úÖ Verifiziert Buchdaten
- ‚úÖ Sehr kosten-effizient

**Anwendungsfall:**
```python
from google import genai
from google.genai import types

client = genai.Client()

# Nutze Google Search als Tool
response = client.models.generate_content(
    model='gemini-2.0-flash-exp',
    contents=f'Finde aktuelle Marktpreise f√ºr ISBN {isbn} in Deutschland',
    config=types.GenerateContentConfig(
        tools=[{"google_search": {}}]  # ‚Üê Aktiviert Google Search
    )
)

# Gemini nutzt automatisch Google Search und gibt strukturierte Antwort
```

**Was es findet:**
- Aktuelle Verkaufspreise
- Verf√ºgbarkeit bei Online-H√§ndlern
- Editionen und Ver√∂ffentlichungsdaten
- Rezensionen und Bewertungen
- Vergleichbare B√ºcher

**Implementierungs-Komplexit√§t:** üü¢ Sehr niedrig (1 Tag)

---

### 3. Vertex AI Search ‚ö†Ô∏è NICHT EMPFOHLEN

**Was es ist:**
Enterprise Search Engine f√ºr strukturierte und unstrukturierte Daten.

**Warum NICHT geeignet:**
- ‚ùå **Ben√∂tigt eigenen Datenindex** - m√ºssen wir selbst bef√ºllen
- ‚ùå **Teuer** - $1.50 pro 1000 Queries
- ‚ùå **Overhead** - komplex zu setup und maintain
- ‚ùå **Overkill** - f√ºr unseren Use Case zu m√§chtig

**Bessere Alternative:** Google Search Grounding (siehe oben)

---

### 4. Vertex AI Agent Builder (Reasoning Engine) ü§î OPTIONAL

**Was es ist:**
Low-code Platform f√ºr Multi-Agent Workflows mit reasoning capabilities.

**Vorteile:**
- ‚úÖ Kann komplexe Multi-Step Recherchen durchf√ºhren
- ‚úÖ Orchestriert mehrere Datenquellen
- ‚úÖ Reasoning √ºber Ergebnisse

**Nachteile:**
- ‚ö†Ô∏è Noch in Preview
- ‚ö†Ô∏è Zus√§tzliche Komplexit√§t
- ‚ö†Ô∏è Wir haben bereits eine funktionierende Agent-Architektur

**Empfehlung:** Sp√§ter evaluieren, wenn grundlegende Integration funktioniert

---

## üèóÔ∏è Implementierungsstrategie

### Phase 1: Gemini Web Scraper (Priorit√§t: HOCH)

**Ziel:** Erweitere Data Fusion Engine mit intelligentem Web Scraping

**Architektur:**
```
Existing Data Sources:
‚îú‚îÄ‚îÄ Google Books API
‚îú‚îÄ‚îÄ OpenLibrary API
‚îî‚îÄ‚îÄ AI Extraction (Gemini Vision)

NEW Data Sources:
‚îú‚îÄ‚îÄ Gemini Web Scraper
‚îÇ   ‚îú‚îÄ‚îÄ eurobuch.de (Preise + Verf√ºgbarkeit)
‚îÇ   ‚îú‚îÄ‚îÄ ZVAB (Antiquarische B√ºcher)
‚îÇ   ‚îî‚îÄ‚îÄ booklooker.de (Gebrauchtmarkt)
‚îî‚îÄ‚îÄ Google Search Grounding
    ‚îú‚îÄ‚îÄ Aktuelle Marktpreise
    ‚îú‚îÄ‚îÄ Neue Editionen
    ‚îî‚îÄ‚îÄ Verifikation
```

**Implementation Plan:**

#### Step 1: Web Scraper Client erstellen
```python
# shared/apis/web_scraper.py

from google import genai
from google.genai import types
import asyncio
from typing import Dict, Any, List, Optional
import logging

logger = logging.getLogger(__name__)

class GeminiWebScraper:
    """
    Intelligenter Web Scraper der Gemini nutzt um Websites zu analysieren.
    """
    
    def __init__(self, project_id: str, region: str = "us-central1"):
        self.client = genai.Client()
        self.model = "gemini-2.0-flash-exp"
    
    async def scrape_eurobuch(self, isbn: str) -> Dict[str, Any]:
        """
        Crawlt eurobuch.de nach Preis- und Verf√ºgbarkeitsinformationen.
        
        Args:
            isbn: ISBN des Buches
            
        Returns:
            Dict mit Preisinformationen, Verf√ºgbarkeit, Zustand
        """
        url = f"https://www.eurobuch.de/buch/isbn/{isbn}.html"
        
        prompt = f"""
        Analysiere diese Webseite und extrahiere Buchinformationen:
        URL: {url}
        
        Extrahiere folgende Daten als JSON:
        {{
          "price_range": {{"min": float, "max": float}},
          "average_price": float,
          "availability": "X Angebote",
          "condition_stats": {{"neu": int, "gebraucht": int, "sehr_gut": int}},
          "sellers": int,
          "editions_found": ["Taschenbuch", "Gebunden", ...],
          "confidence": 0.0-1.0
        }}
        
        Wenn die Seite nicht existiert oder keine Daten hat, gib confidence=0.0 zur√ºck.
        """
        
        try:
            # Nutze Gemini mit URL Grounding
            response = await self.client.models.generate_content_async(
                model=self.model,
                contents=[prompt],
                config=types.GenerateContentConfig(
                    temperature=0.1,  # Niedrig f√ºr faktische Extraktion
                    response_mime_type="application/json"
                )
            )
            
            result = json.loads(response.text)
            logger.info(f"Eurobuch scrape successful for ISBN {isbn}")
            return result
            
        except Exception as e:
            logger.error(f"Eurobuch scrape failed for {isbn}: {e}")
            return {"confidence": 0.0, "error": str(e)}
    
    async def scrape_zvab(self, title: str, author: str) -> Dict[str, Any]:
        """Crawlt ZVAB nach antiquarischen Angeboten."""
        # √Ñhnliche Implementation wie eurobuch
        pass
    
    async def scrape_multiple_sources(
        self, 
        isbn: Optional[str] = None,
        title: Optional[str] = None,
        author: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Crawlt mehrere Quellen parallel.
        
        Returns:
            Liste von Ergebnissen von verschiedenen Quellen
        """
        tasks = []
        
        if isbn:
            tasks.append(self.scrape_eurobuch(isbn))
            # tasks.append(self.scrape_zvab_by_isbn(isbn))
        
        if title and author:
            # tasks.append(self.scrape_zvab(title, author))
            # tasks.append(self.scrape_booklooker(title, author))
            pass
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter erfolgreiche Ergebnisse
        return [r for r in results if isinstance(r, dict) and r.get("confidence", 0) > 0.3]
```

#### Step 2: Google Search Grounding hinzuf√ºgen
```python
# shared/apis/search_grounding.py

from google import genai
from google.genai import types
import logging

logger = logging.getLogger(__name__)

class GoogleSearchGrounding:
    """
    Nutzt Google Search Grounding f√ºr aktuelle Marktdaten.
    """
    
    def __init__(self):
        self.client = genai.Client()
        self.model = "gemini-2.0-flash-exp"
    
    async def search_market_prices(
        self, 
        isbn: str,
        title: str,
        author: str
    ) -> Dict[str, Any]:
        """
        Nutzt Google Search um aktuelle Marktpreise zu finden.
        
        Returns:
            Dict mit Preisinformationen und Quellen
        """
        prompt = f"""
        Finde aktuelle Marktpreise f√ºr dieses Buch:
        - ISBN: {isbn}
        - Titel: {title}
        - Autor: {author}
        
        Suche nach:
        1. Verkaufspreisen bei deutschen Online-Buchh√§ndlern
        2. Gebrauchtpreisen
        3. Verf√ºgbarkeit
        
        Gib die Ergebnisse als JSON zur√ºck:
        {{
          "new_price_range": {{"min": float, "max": float}},
          "used_price_range": {{"min": float, "max": float}},
          "availability": "string",
          "sources": ["source1", "source2", ...],
          "confidence": 0.0-1.0
        }}
        """
        
        try:
            response = await self.client.models.generate_content_async(
                model=self.model,
                contents=[prompt],
                config=types.GenerateContentConfig(
                    tools=[{"google_search": {}}],  # ‚Üê Google Search aktiviert
                    response_mime_type="application/json"
                )
            )
            
            result = json.loads(response.text)
            
            # Log welche Seiten verwendet wurden
            if response.candidates[0].grounding_metadata:
                search_queries = response.candidates[0].grounding_metadata.web_search_queries
                logger.info(f"Search queries used: {search_queries}")
            
            return result
            
        except Exception as e:
            logger.error(f"Search grounding failed: {e}")
            return {"confidence": 0.0, "error": str(e)}
    
    async def verify_book_data(
        self,
        existing_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Verifiziert vorhandene Buchdaten mit Google Search.
        
        N√ºtzlich um:
        - Falsche Editionen zu erkennen
        - Veraltete Informationen zu updaten
        - Neue Editionen zu entdecken
        """
        prompt = f"""
        Verifiziere diese Buchdaten:
        {json.dumps(existing_data, indent=2)}
        
        Pr√ºfe ob:
        1. Titel und Autor korrekt sind
        2. Das Erscheinungsjahr stimmt
        3. Die Edition korrekt ist
        4. Es neuere Editionen gibt
        
        Gib zur√ºck:
        {{
          "verified": boolean,
          "corrections": {{"field": "corrected_value", ...}},
          "new_editions_found": [...],
          "confidence": 0.0-1.0
        }}
        """
        
        # Implementation √§hnlich zu search_market_prices
        pass
```

#### Step 3: Data Fusion Engine erweitern
```python
# In shared/apis/data_fusion.py

class DataFusionEngine:
    def __init__(self):
        # Existing clients
        self.openlibrary_client = OpenLibraryClient()
        self.google_books_client = GoogleBooksClient(...)
        
        # NEW: Web Scraping + Search
        self.web_scraper = GeminiWebScraper(project_id=GCP_PROJECT)
        self.search_grounding = GoogleSearchGrounding()
    
    async def enhanced_book_research_v2(
        self,
        base_data: Dict[str, Any],
        image_urls: List[str]
    ) -> Dict[str, Any]:
        """
        V2 mit Web Scraping und Search Grounding.
        """
        # Existing research (Google Books + OpenLibrary + AI)
        existing_result = await self.enhanced_book_research(base_data, image_urls)
        
        # NEW: Web Scraping
        isbn = base_data.get('isbn')
        title = base_data.get('title')
        author = base_data.get('author')
        
        scraping_results = []
        if isbn:
            scraping_results = await self.web_scraper.scrape_multiple_sources(
                isbn=isbn, 
                title=title, 
                author=author
            )
        
        # NEW: Google Search Grounding
        search_result = await self.search_grounding.search_market_prices(
            isbn=isbn,
            title=title,
            author=author
        )
        
        # Fuse all data sources
        fused_data = self._fuse_all_sources(
            existing_result,
            scraping_results,
            search_result
        )
        
        # Enhanced metadata
        fused_data["_metadata"]["sources_used"].extend([
            "eurobuch_scraping",
            "google_search_grounding"
        ])
        
        # Recalculate confidence with new sources
        fused_data["confidence_score"] = self._calculate_enhanced_confidence(
            fused_data
        )
        
        return fused_data
    
    def _calculate_enhanced_confidence(self, data: Dict[str, Any]) -> float:
        """
        Erweiterte Confidence-Berechnung mit Web Scraping.
        
        Source Weights:
        - Google Books: 1.0
        - OpenLibrary: 0.9
        - Web Scraping (eurobuch): 0.85
        - Google Search Grounding: 0.8
        - AI Extraction: 0.4
        """
        weights = {
            "google_books": 1.0,
            "openlibrary": 0.9,
            "eurobuch_scraping": 0.85,
            "google_search_grounding": 0.8,
            "ai_extraction": 0.4
        }
        
        sources_used = data["_metadata"].get("sources_used", [])
        
        if not sources_used:
            return 0.0
        
        # Weighted average
        total_weight = sum(weights.get(s, 0.5) for s in sources_used)
        confidence = total_weight / len(sources_used)
        
        return min(confidence, 1.0)
```

---

## üí∞ Kosten-Analyse

### Gemini Web Scraping

**Kosten pro Buch:**
- Gemini 2.0 Flash: $0.000075 per 1K input tokens
- Eurobuch-Seite: ~5K tokens
- ZVAB-Seite: ~5K tokens
- Parsing: ~2K output tokens

**Gesamt: ~$0.001 pro Buch** (sehr g√ºnstig!)

### Google Search Grounding

**Kosten:**
- Inkludiert in Gemini API Kosten
- Keine zus√§tzlichen Charges f√ºr Search
- ~1K output tokens: $0.0003

**Gesamt: ~$0.0003 pro Search** (extrem g√ºnstig!)

### Vergleich zu Vertex AI Search

**Vertex AI Search:**
- $1.50 per 1000 queries
- **= $0.0015 pro Query** (5x teurer als unsere L√∂sung)
- Plus Setup und Maintenance Overhead

**Empfehlung:** Gemini Web Scraping + Search Grounding ist **deutlich g√ºnstiger und flexibler**

---

## üìä Erwartete Verbesserungen

### Datenqualit√§t

**Vor Vertex AI Integration:**
- Confidence bei ISBN-Match: ~0.85
- Confidence ohne ISBN: ~0.6
- Editions-Genauigkeit: ~70%

**Nach Vertex AI Integration:**
- Confidence bei ISBN-Match: ~0.92 (+7%)
- Confidence ohne ISBN: ~0.75 (+15%)
- Editions-Genauigkeit: ~85% (+15%)

### Neue Datenfelder

**Preisinformationen:**
```json
{
  "market_data": {
    "new_price_range": {"min": 12.99, "max": 19.99},
    "used_price_range": {"min": 4.99, "max": 9.99},
    "average_market_price": 8.50,
    "availability": "gut verf√ºgbar",
    "sellers_count": 27
  }
}
```

**Editions-Details:**
```json
{
  "edition_analysis": {
    "detected_edition": "dtv Taschenbuch, 5. Auflage",
    "other_editions_available": [
      "Gebundene Ausgabe (Erstausgabe)",
      "E-Book",
      "H√∂rbuch"
    ],
    "publication_history": [
      {"year": 2015, "edition": "Erstausgabe"},
      {"year": 2020, "edition": "Taschenbuch"}
    ]
  }
}
```

---

## üöÄ Implementation Timeline

### Week 1: Foundation
- [ ] GeminiWebScraper Client erstellen
- [ ] GoogleSearchGrounding Client erstellen
- [ ] Unit Tests schreiben
- [ ] Basic Integration in Data Fusion Engine

### Week 2: Integration
- [ ] Eurobuch.de Scraping implementieren
- [ ] Google Search Grounding integrieren
- [ ] Enhanced Confidence Scoring
- [ ] End-to-End Testing

### Week 3: Expansion
- [ ] ZVAB Scraping hinzuf√ºgen
- [ ] Booklooker.de optional
- [ ] Performance Optimierung
- [ ] Documentation

### Week 4: Production
- [ ] Load Testing
- [ ] Error Handling Refinement
- [ ] Deploy to Alpha
- [ ] Monitor und Optimize

---

## ‚ö†Ô∏è Considerations & Risks

### Technisch

1. **Rate Limiting**
   - Eurobuch.de k√∂nnte Rate Limits haben
   - **L√∂sung:** Caching, delays zwischen Requests

2. **Website Changes**
   - Websites k√∂nnen ihre Struktur √§ndern
   - **L√∂sung:** Gemini ist robust gegen kleine √Ñnderungen

3. **Kosten**
   - Mehr API Calls = h√∂here Kosten
   - **L√∂sung:** Caching, nur bei Bedarf scrapen

### Legal

1. **Web Scraping Legalit√§t**
   - Muss robots.txt respektieren
   - Keine pers√∂nlichen Daten sammeln
   - **L√∂sung:** Nur √∂ffentliche Produktdaten

2. **Terms of Service**
   - Websites k√∂nnten Scraping verbieten
   - **L√∂sung:** Pr√ºfen, ggf. APIs nutzen

---

## üéØ Empfehlung

**Start mit:**
1. ‚úÖ **Gemini Web Scraper** f√ºr eurobuch.de
2. ‚úÖ **Google Search Grounding** f√ºr Verifikation

**Sp√§ter evaluieren:**
- ZVAB Scraping
- Booklooker.de
- Amazon Product API (wenn verf√ºgbar)

**NICHT nutzen:**
- ‚ùå Vertex AI Search (Overkill und teuer)
- ‚ùå Vertex AI Agent Builder (noch zu komplex)

---

## üìö N√§chste Schritte

1. **Review dieses Dokuments** mit dem Team
2. **Entscheidung:** Welche Quellen zuerst implementieren?
3. **Prototype erstellen:** GeminiWebScraper + Eurobuch
4. **Testing:** Mit echten ISBNs
5. **Integration:** In Data Fusion Engine
6. **Deploy:** Alpha-Launch

---

**Dokument erstellt:** 2025-11-04  
**Version:** 1.0  
**Status:** Proposal - Awaiting Approval  
**N√§chster Review:** Nach Prototype Implementation